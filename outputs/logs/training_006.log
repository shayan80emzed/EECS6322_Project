INFO - ================================================================================
INFO - Starting CLIP Vision Encoder Adversarial Fine-tuning
INFO - ================================================================================
INFO - Log file: outputs/logs/training_006.log
INFO - Python version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]
INFO - PyTorch version: 2.0.1+cu117
INFO - CUDA available: True
INFO - CUDA device: Quadro RTX 6000
INFO - Number of GPUs: 4
INFO - --------------------------------------------------------------------------------
INFO - Training Configuration:
INFO - --------------------------------------------------------------------------------
INFO - clip_model_name          : ViT-B-16
INFO - pretrained               : openai
INFO - dataset                  : imagenet
INFO - imagenet_root            : /datasets/imagenet
INFO - steps                    : 600
INFO - warmup                   : 400
INFO - batch_size               : 128
INFO - lr                       : 0.0001
INFO - wd                       : 0.0001
INFO - opt                      : adamw
INFO - momentum_sgd             : 0.9
INFO - attack                   : pgd
INFO - norm                     : huber
INFO - eps                      : 0.01568627450980392
INFO - eps2                     : 0.01568627450980392
INFO - iterations_adv           : 10
INFO - stepsize_adv             : 0.00392156862745098
INFO - clean_weight             : 0.0
INFO - output_dir               : outputs
INFO - save_checkpoints         : True
INFO - log_freq                 : 30
INFO - log_file                 : outputs/training.log
INFO - output_normalize         : False
INFO - --------------------------------------------------------------------------------
INFO - Initializing training...
INFO - Successfully loaded CLIP model
INFO - Successfully loaded CLIP model
INFO - Loading dataset from: /datasets/imagenet
INFO - Training samples: 1281167
INFO - Created dataloader with batch size: 128
INFO - Setting up models...
INFO - Models moved to CUDA
INFO - Using DataParallel with 4 GPUs
INFO - Setting up adamw optimizer...
INFO - Optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001
)
INFO - Setting up cosine learning rate scheduler...
INFO - Learning rate scheduler: cosine with warmup=400, total_steps=600
INFO - Starting training loop...
INFO - Train Step: 0/600 (0%)	Loss: 0.332318
INFO - ================================================================================
INFO - Training finished
INFO - ================================================================================
