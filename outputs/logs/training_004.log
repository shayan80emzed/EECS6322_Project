INFO - ================================================================================
INFO - Starting CLIP Vision Encoder Adversarial Fine-tuning
INFO - ================================================================================
INFO - Log file: outputs/logs/training_004.log
INFO - Python version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]
INFO - PyTorch version: 2.0.1+cu117
INFO - CUDA available: True
INFO - CUDA device: Quadro RTX 6000
INFO - Number of GPUs: 4
INFO - --------------------------------------------------------------------------------
INFO - Training Configuration:
INFO - --------------------------------------------------------------------------------
INFO - clip_model_name          : ViT-B-16
INFO - pretrained               : openai
INFO - dataset                  : imagenet
INFO - imagenet_root            : /datasets/imagenet
INFO - steps                    : 2000
INFO - warmup                   : 1400
INFO - batch_size               : 128
INFO - lr                       : 0.0001
INFO - wd                       : 0.0001
INFO - opt                      : adamw
INFO - momentum_sgd             : 0.9
INFO - attack                   : pgd
INFO - norm                     : linf
INFO - eps                      : 0.01568627450980392
INFO - eps2                     : 0.01568627450980392
INFO - iterations_adv           : 10
INFO - stepsize_adv             : 0.00392156862745098
INFO - clean_weight             : 0.0
INFO - output_dir               : outputs
INFO - save_checkpoints         : True
INFO - log_freq                 : 30
INFO - log_file                 : outputs/training.log
INFO - output_normalize         : False
INFO - --------------------------------------------------------------------------------
INFO - Initializing training...
INFO - Successfully loaded CLIP model
INFO - Successfully loaded CLIP model
INFO - Loading dataset from: /datasets/imagenet
INFO - Training samples: 1281167
INFO - Created dataloader with batch size: 128
INFO - Setting up models...
INFO - Models moved to CUDA
INFO - Using DataParallel with 4 GPUs
INFO - Setting up adamw optimizer...
INFO - Optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001
)
INFO - Setting up cosine learning rate scheduler...
INFO - Learning rate scheduler: cosine with warmup=1400, total_steps=2000
INFO - Starting training loop...
INFO - Train Step: 0/2000 (0%)	Loss: 0.331985
INFO - Train Step: 30/2000 (2%)	Loss: 0.275950
INFO - Train Step: 60/2000 (3%)	Loss: 0.123050
INFO - Train Step: 90/2000 (4%)	Loss: 0.107544
INFO - Train Step: 120/2000 (6%)	Loss: 0.107513
INFO - Train Step: 150/2000 (8%)	Loss: 0.102709
INFO - Train Step: 180/2000 (9%)	Loss: 0.099589
INFO - Train Step: 210/2000 (10%)	Loss: 0.094150
INFO - Train Step: 240/2000 (12%)	Loss: 0.087778
INFO - Train Step: 270/2000 (14%)	Loss: 0.080832
INFO - Train Step: 300/2000 (15%)	Loss: 0.083082
INFO - Train Step: 330/2000 (16%)	Loss: 0.078625
INFO - Train Step: 360/2000 (18%)	Loss: 0.078340
INFO - Train Step: 390/2000 (20%)	Loss: 0.074084
INFO - Train Step: 420/2000 (21%)	Loss: 0.076446
INFO - Train Step: 450/2000 (22%)	Loss: 0.072650
INFO - Train Step: 480/2000 (24%)	Loss: 0.073693
INFO - Train Step: 510/2000 (26%)	Loss: 0.074437
INFO - Train Step: 540/2000 (27%)	Loss: 0.075803
INFO - Train Step: 570/2000 (28%)	Loss: 0.071682
INFO - Train Step: 600/2000 (30%)	Loss: 0.069181
INFO - Train Step: 630/2000 (32%)	Loss: 0.075793
INFO - Train Step: 660/2000 (33%)	Loss: 0.071300
INFO - Train Step: 690/2000 (34%)	Loss: 0.072362
INFO - Train Step: 720/2000 (36%)	Loss: 0.071136
INFO - Train Step: 750/2000 (38%)	Loss: 0.070537
INFO - Train Step: 780/2000 (39%)	Loss: 0.070421
INFO - Train Step: 810/2000 (40%)	Loss: 0.074205
INFO - Train Step: 840/2000 (42%)	Loss: 0.070911
INFO - Train Step: 870/2000 (44%)	Loss: 0.069157
INFO - Train Step: 900/2000 (45%)	Loss: 0.071017
INFO - Train Step: 930/2000 (46%)	Loss: 0.066200
INFO - Train Step: 960/2000 (48%)	Loss: 0.069571
INFO - Train Step: 990/2000 (50%)	Loss: 0.070299
INFO - Train Step: 1020/2000 (51%)	Loss: 0.069518
INFO - Train Step: 1050/2000 (52%)	Loss: 0.070695
INFO - Train Step: 1080/2000 (54%)	Loss: 0.068147
INFO - Train Step: 1110/2000 (56%)	Loss: 0.072209
INFO - Train Step: 1140/2000 (57%)	Loss: 0.069899
INFO - Train Step: 1170/2000 (58%)	Loss: 0.069441
INFO - Train Step: 1200/2000 (60%)	Loss: 0.073318
INFO - Train Step: 1230/2000 (62%)	Loss: 0.071861
INFO - Train Step: 1260/2000 (63%)	Loss: 0.070710
INFO - Train Step: 1290/2000 (64%)	Loss: 0.071421
INFO - Train Step: 1320/2000 (66%)	Loss: 0.068790
INFO - Train Step: 1350/2000 (68%)	Loss: 0.069778
INFO - Train Step: 1380/2000 (69%)	Loss: 0.066620
INFO - Train Step: 1410/2000 (70%)	Loss: 0.070630
INFO - Train Step: 1440/2000 (72%)	Loss: 0.068460
INFO - Train Step: 1470/2000 (74%)	Loss: 0.070602
INFO - Train Step: 1500/2000 (75%)	Loss: 0.069835
INFO - Train Step: 1530/2000 (76%)	Loss: 0.071924
INFO - Train Step: 1560/2000 (78%)	Loss: 0.068054
INFO - Train Step: 1590/2000 (80%)	Loss: 0.068578
INFO - Train Step: 1620/2000 (81%)	Loss: 0.067606
