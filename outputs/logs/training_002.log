INFO - ================================================================================
INFO - Starting CLIP Vision Encoder Adversarial Fine-tuning
INFO - ================================================================================
INFO - Log file: outputs/logs/training_002.log
INFO - Python version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]
INFO - PyTorch version: 2.0.1+cu117
INFO - CUDA available: True
INFO - CUDA device: Quadro RTX 6000
INFO - Number of GPUs: 4
INFO - --------------------------------------------------------------------------------
INFO - Training Configuration:
INFO - --------------------------------------------------------------------------------
INFO - clip_model_name          : ViT-B-16
INFO - pretrained               : openai
INFO - dataset                  : imagenet
INFO - imagenet_root            : /datasets/imagenet
INFO - steps                    : 2000
INFO - warmup                   : 1400
INFO - batch_size               : 128
INFO - lr                       : 0.0001
INFO - wd                       : 0.0001
INFO - opt                      : adamw
INFO - momentum_sgd             : 0.9
INFO - attack                   : pgd
INFO - norm                     : linf
INFO - eps                      : 0.01568627450980392
INFO - eps2                     : 0.01568627450980392
INFO - iterations_adv           : 10
INFO - stepsize_adv             : 0.00392156862745098
INFO - clean_weight             : 0.0
INFO - output_dir               : outputs
INFO - save_checkpoints         : True
INFO - log_freq                 : 30
INFO - log_file                 : outputs/training.log
INFO - output_normalize         : False
INFO - --------------------------------------------------------------------------------
INFO - Initializing training...
INFO - Successfully loaded CLIP model
INFO - Successfully loaded CLIP model
INFO - Loading dataset from: /datasets/imagenet
INFO - Training samples: 1281167
INFO - Created dataloader with batch size: 128
INFO - Setting up models...
INFO - Models moved to CUDA
INFO - Using DataParallel with 4 GPUs
INFO - Setting up adamw optimizer...
INFO - Optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001
)
INFO - Setting up cosine learning rate scheduler...
INFO - Learning rate scheduler: cosine with warmup=1400, total_steps=2000
INFO - Starting training loop...
ERROR - Training failed with error: train_one_epoch.<locals>.<lambda>() takes 1 positional argument but 2 were given
Traceback (most recent call last):
  File "/fs01/home/emzed/eecs6322/Project/clip_aft/main.py", line 18, in main
    train(config)
  File "/fs01/home/emzed/eecs6322/Project/clip_aft/train.py", line 115, in train
    step_total = train_one_epoch(
  File "/fs01/home/emzed/eecs6322/Project/clip_aft/train.py", line 28, in train_one_epoch
    data_adv = pgd(
  File "/fs01/home/emzed/eecs6322/Project/clip_aft/attacks/pgd.py", line 57, in pgd
    loss = loss_fn(out, targets)
TypeError: train_one_epoch.<locals>.<lambda>() takes 1 positional argument but 2 were given
INFO - ================================================================================
INFO - Training finished
INFO - ================================================================================
