# ğŸ§  EECS6322 Project: Boosting the Visual Interpretability of CLIP via Adversarial Fine-Tuning

> Reproducibility Study Â· York University  
> Course: EECS6322 - Neural Networks and Deep Learning  
> Author: Shayan Mohammadizadehsamakosh

---

## ğŸ“Œ Overview

This project reproduces the results from the paper  
**"Boosting the Visual Interpretability of CLIP via Adversarial Fine-Tuning"**.

We implement and test the proposed **Adversarial Fine-Tuning (AFT)** framework to improve the **visual interpretability** of the CLIP modelâ€™s vision encoder, without using any labels or modifying the text encoder.

- Fine-tuning is done adversarially on images only (ImageNet).
- Evaluations include saliency maps, attention maps, and ROAR analysis.
- Results confirm improved interpretability, aligning with the original paperâ€™s claims.

---

## ğŸ—‚ï¸ Project Structure

```bash
COMPLETE TREE

.
â”œâ”€â”€ main.py                # Entry point: sets up config, logger, and runs training
â”œâ”€â”€ train.py               # Main training loop and multi-GPU support
â”œâ”€â”€ pgd_huber.py           # PGD implementation with Huber and L-infinity norms
â”œâ”€â”€ config.py              # Config files for training runs (dataclass)
â”œâ”€â”€ outputs/               # Directory to store trained model weights and loggings
â”‚   â”œâ”€â”€ logs/              # Loggings during the training
â”‚   â””â”€â”€ checkpoints/       # Will be created if you try training the model (not included because the .pt files are huge!)
â”œâ”€â”€ results/               # Contains saliency maps, attention visualizations, ROAR outputs
â”‚   â”œâ”€â”€ SG_GC_ATT.ipynb    # Pipeline for feature attribution by Simple Gradient and GradCAM, plus visualization of attention maps of ViT
â”‚   â””â”€â”€ ROAR/              # Pipleline for feature importance analysis
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project documentation (this file)


```
